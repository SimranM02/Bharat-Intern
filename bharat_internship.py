# -*- coding: utf-8 -*-
"""Bharat Internship.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gbfg7Qz2rZMNBppRc8editUuQ3d5MhUd

# upload
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip "/content/drive/MyDrive/Colab Notebooks/archive.zip" -d "/content/"

"""# cat-dog analysis"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, datasets
import os
from PIL import Image
import numpy as np

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)
        self.fc1 = nn.Linear(64 * 64 * 64, 512)
        self.fc2 = nn.Linear(512, 2)

    def forward(self, x):
        x = nn.functional.relu(self.conv1(x))
        x = nn.functional.relu(self.conv2(x))
        x = nn.functional.relu(self.conv3(x))
        x = x.view(-1, 64 * 64 * 64)
        x = nn.functional.relu(self.fc1(x))
        x = self.fc2(x)
        return nn.functional.softmax(x, dim=1)

class CartoonDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.images = os.listdir(root_dir)

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_name = os.path.join(self.root_dir, self.images[idx])
        image = Image.open(img_name).convert("RGB")
        if self.transform:
            image = self.transform(image)
        label = 0 if "cat" in self.images[idx] else 1
        return image, label

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

train_data = CartoonDataset(root_dir='/content/archive/cats_and_dogs/cats_and_dogs', transform=transform)
train_loader = DataLoader(train_data, batch_size=8, shuffle=True)

model = CNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

num_epochs = 10

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')

print('Finished Training')

torch.save(model.state_dict(), 'dog_cat_classifier.pth')

model = CNN()
model.load_state_dict(torch.load('dog_cat_classifier.pth'))
model.eval()

def predict_image(image_path):
    image = Image.open(image_path).convert("RGB")
    image_tensor = transform(image).unsqueeze(0)

    with torch.no_grad():
        output = model(image_tensor)
        _, predicted = torch.max(output, 1)

        if predicted.item() == 0:
            prediction = "It's a cat üêà"
        else:
            prediction = "It's a dog üêï"

        plt.imshow(image)
        plt.title(prediction)
        plt.axis('off')
        plt.show()

predict_image('/content/archive/cats_and_dogs/Test/Test 2.png')

"""# new code cda"""

import torch
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from PIL import Image
import os

# Custom dataset class to handle filenames with labels
class CustomDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.file_list = os.listdir(data_dir)

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        img_name = os.path.join(self.data_dir, self.file_list[idx])
        image = Image.open(img_name).convert('RGB')
        label = 0 if "cat_" in self.file_list[idx] else 1  # Assuming "cat_" means class 0, "dog_" means class 1

        if self.transform:
            image = self.transform(image)

        return image, label

data_dir = "/content/archive/cats_and_dogs/cats_and_dogs"
batch_size = 32

# Define transformations
transform = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor(),
])

# Create custom dataset
custom_dataset = CustomDataset(data_dir, transform=transform)

# Create data loader
train_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.conv3 = nn.Conv2d(64, 128, 3, 1)
        self.pool = nn.MaxPool2d(2, 2)  # Pooling layer to reduce spatial dimensions
        self.fc1 = nn.Linear(128*29*29, 128)  # Adjusted input size based on the output of the pooling layer
        self.fc2 = nn.Linear(128, 2)  # 2 classes: cat and dog

    def forward(self, x):
        print("Input shape:", x.shape)
        x = nn.functional.relu(self.conv1(x))
        print("After conv1 shape:", x.shape)
        x = nn.functional.relu(self.conv2(x))
        print("After conv2 shape:", x.shape)
        x = nn.functional.relu(self.conv3(x))
        print("After conv3 shape:", x.shape)
        x = self.pool(x)  # Apply pooling
        print("After pooling shape:", x.shape)
        x = x.view(-1, 128 * 29 * 29)  # Adjusted the input size
        print("After view shape:", x.shape)
        x = nn.functional.relu(self.fc1(x))
        print("After fc1 shape:", x.shape)
        x = self.fc2(x)
        return x

# Instantiate the model
model = CNN()

# Define Loss Function and Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training the Model
epochs = 10

for epoch in range(epochs):
    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        inputs, labels = data
        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if i % 100 == 99:    # Print every 100 mini-batches
            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))
            running_loss = 0.0

print("Finished Training")

# Testing the Model
# You can define a separate test dataset or use part of your existing dataset as a test set

# Function to calculate accuracy
def get_accuracy(model, data_loader):
    correct = 0
    total = 0
    with torch.no_grad():
        for data in data_loader:
            images, labels = data
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return correct / total

# Test the model on a test set (or part of the dataset)
# Here, we'll just use the same dataset as the test set
test_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=False)

# Test the model
model.eval()  # Set model to evaluation mode
test_accuracy = get_accuracy(model, test_loader)
print(f"Test Accuracy: {test_accuracy}")

"""# Titanic Classification"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

titanic_data = pd.read_csv("titanic.csv")

print(titanic_data.head())
print(titanic_data.info())
print(titanic_data.describe())

imputer = SimpleImputer(strategy="mean")
titanic_data["Age"] = imputer.fit_transform(titanic_data[["Age"]])

titanic_data = pd.get_dummies(titanic_data, columns=["Sex", "Embarked"], drop_first=True)

X = titanic_data.drop(["Survived", "PassengerId", "Name", "Ticket", "Cabin"], axis=1)
y = titanic_data["Survived"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = RandomForestClassifier(random_state=42)
model.fit(X_train_scaled, y_train)

y_pred = model.predict(X_test_scaled)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

probabilities = model.predict_proba(X_test_scaled)[:, 1]

plt.figure(figsize=(10, 6))
plt.hist(probabilities, bins=20, color='skyblue', edgecolor='black')
plt.xlabel('Predicted Probability of Survival')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted Probabilities of Survival')
plt.grid(True)
plt.show()